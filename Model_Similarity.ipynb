{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "858b300f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05c7a7d-74fa-40ef-cbe2-00fc21ad6a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/mr_modares/.conda/envs/thesis/lib/python3.10/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/home/mr_modares/.conda/envs/thesis/lib/python3.10/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import matmul, exp, log, abs\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import AutoTokenizer,AutoModelForQuestionAnswering, AutoModelForMultipleChoice, AutoModelForSequenceClassification, AutoModel, BertModel, ElectraModel, set_seed\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers.modeling_outputs import TokenClassifierOutput, SequenceClassifierOutput\n",
        "from transformers import AdamW,get_scheduler\n",
        "\n",
        "from datasets import load_metric, load_dataset\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import math\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftConfig, PeftModel\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy import linalg as LA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb574ec8"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC66Vv8WE6w1",
        "outputId": "0e4af418-7a65-48d2-a12d-9d79bd822874"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a539faf"
      },
      "outputs": [],
      "source": [
        "# Setting seed\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(seed = SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cdf78b7"
      },
      "outputs": [],
      "source": [
        "# language model hyper parameters\n",
        "batch_size = 40\n",
        "epochs = 3\n",
        "lr = 1e-5\n",
        "weight_decay = 0.1\n",
        "\n",
        "ALPHA=1.0\n",
        "RANK=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models fine-tuned with LoRA"
      ],
      "metadata": {
        "id": "FOPCEmDQ_BvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sst2_42_model_id = './models/bert_lora_sst2_42'\n",
        "imdb_model_id = './models/bert_lora_imdb_42'\n",
        "\n",
        "sst2_42_config = PeftConfig.from_pretrained(sst2_42_model_id)\n",
        "imdb_config = PeftConfig.from_pretrained(imdb_model_id)\n",
        "\n",
        "sst2_42_model = AutoModelForSequenceClassification.from_pretrained(sst2_42_config.base_model_name_or_path)\n",
        "sst2_42_model = PeftModel.from_pretrained(sst2_42_model, sst2_42_model_id)\n",
        "\n",
        "imdb_model = AutoModelForSequenceClassification.from_pretrained(imdb_config.base_model_name_or_path)\n",
        "imdb_model = PeftModel.from_pretrained(imdb_model, imdb_model_id)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(sst2_28_config.base_model_name_or_path)"
      ],
      "metadata": {
        "id": "6TLB2X_bCK7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grassman Similarity"
      ],
      "metadata": {
        "id": "90Ah2vdfCXNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grassman_dist(u_A, u_B, i, j):\n",
        "\n",
        "    # print(u_A[:, :i].T.shape, u_B[:, :j].shape)A\n",
        "    # return (LA.norm(np.matmul(u_A[:, :i].T, u_B[:, :j])) ** 2) / min(i, j)\n",
        "    return torch.div(torch.pow(torch.norm(torch.matmul(u_A[:, :i].T, u_B[:, :j]), p='fro'), 2), min(i, j)).numpy()"
      ],
      "metadata": {
        "id": "Cwt46WDAC74-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define plotting function"
      ],
      "metadata": {
        "id": "iVrkmOikgFx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_all_layers(modelA: nn.Module, modelB: nn.Module, max_rank: int, title: str):\n",
        "    # Create a 3x4 grid of subplots\n",
        "    fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(11, 9))\n",
        "\n",
        "    # to store max similarity\n",
        "    vmax = 0\n",
        "    sumation=0\n",
        "    all_layers = np.zeros((12, 4, 4))\n",
        "    # Loop over each subplot and plot a heatmap\n",
        "    for l, ax in enumerate(axs.flat):\n",
        "        lora_matrix_1 = torch.matmul(\n",
        "            modelA.base_model.bert.encoder.layer[l].attention.self.value.lora_B.default.weight,\n",
        "            modelA.base_model.bert.encoder.layer[l].attention.self.value.lora_A.default.weight).detach()\n",
        "        lora_matrix_2= torch.matmul(\n",
        "            modelB.base_model.bert.encoder.layer[l].attention.self.value.lora_B.default.weight,\n",
        "            modelB.base_model.bert.encoder.layer[l].attention.self.value.lora_A.default.weight).detach()\n",
        "\n",
        "        # empty matrix to store grassman distance for different i and j\n",
        "        dist_matrix = np.zeros((max_rank, max_rank))\n",
        "\n",
        "        # SVD decomposition\n",
        "        u_A, s_A, v_A = torch.linalg.svd(lora_matrix_1)\n",
        "        u_B, s_B, v_B = torch.linalg.svd(lora_matrix_2)\n",
        "\n",
        "        # calculate grassman dist for different i and j\n",
        "        for i in range(1, max_rank+1):\n",
        "            for j in range(1, max_rank+1):\n",
        "                dist_matrix[i-1][j-1] = grassman_dist(u_A, u_B, i, j)\n",
        "\n",
        "        vmax = max(np.max(dist_matrix), vmax)\n",
        "        sumation+= np.max(dist_matrix)\n",
        "        all_layers[l] = dist_matrix\n",
        "        # Plot the heatmap on the current subplot\n",
        "        im = ax.imshow(dist_matrix, cmap='hot',  vmin=0, vmax=0.5)\n",
        "        # print(dist_matrix)\n",
        "        # print(np.max(dist_matrix))\n",
        "        # Add a title to the subplot\n",
        "        ax.set_title(f'L{l}: {np.max(dist_matrix):.2f}', fontsize=20)\n",
        "        ax.set_xticks(range(4))\n",
        "        ax.set_yticks(range(4))\n",
        "        ax.set_xticklabels(range(1, 5))\n",
        "        ax.set_yticklabels(range(1, 5))\n",
        "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "    # Add a colorbar to the figure\n",
        "    # fig.colorbar(im, ax=axs.ravel().tolist())\n",
        "    # Create a colorbar\n",
        "    cbar = fig.colorbar(im, ax=axs.ravel().tolist())\n",
        "\n",
        "    # Change the font size of the color bar\n",
        "    cbar.ax.tick_params(labelsize=20)\n",
        "    # fig.suptitle(title)\n",
        "    avg = sumation/12\n",
        "    print(avg)\n",
        "\n",
        "    plt.show()\n",
        "    # return all_layers, avg"
      ],
      "metadata": {
        "id": "NnAAIGOKGh-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_all_layers(sst2_42_model, imdb_model, 4, 'sst2 vs imdb')"
      ],
      "metadata": {
        "id": "PXr7tZUS2nf7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}